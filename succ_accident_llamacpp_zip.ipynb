{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCIXi4v2Tow"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxR36fPS9ZXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تم تشغيل لاما من مجلد تم ضغطه بعد البناء وتحميله ولم يعمل الا من خلالا الامر\n",
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HU9f7XKBHdk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "id": "1foSlv04HiRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoNQivvp9Zac",
        "outputId": "3f970c7f-f090-4abc-f592-b5ced7ef106d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-22 00:01:51--  https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.19, 3.169.137.5, 3.169.137.119, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/8d/e8/8de8c6dd40cd4e7b492fc1fc1538dafc96a69f82e46d5c29c8f88fe0d8445713/7b99832b3040f184ab3929409de0094bd5edcc485d6343f82bc861fd4c0c308a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%3B+filename%3D%22DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%22%3B&Expires=1740186112&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4NjExMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzhkL2U4LzhkZThjNmRkNDBjZDRlN2I0OTJmYzFmYzE1MzhkYWZjOTZhNjlmODJlNDZkNWMyOWM4Zjg4ZmUwZDg0NDU3MTMvN2I5OTgzMmIzMDQwZjE4NGFiMzkyOTQwOWRlMDA5NGJkNWVkY2M0ODVkNjM0M2Y4MmJjODYxZmQ0YzBjMzA4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fNeoVjZrfaFCG8QJPHbrocJDnuDb92kjFHcJ4vsO6KM-LtTu71BBzN4gJ0fy4Nkm0HsBujxUaseRLn%7EPZqno1%7Ewyxl5bqaUHUEw3GgTuR4eCBxcQSkf-CzkxSNtuqDbkLzryouPg08Xo4oo8pyR343KgESin2zIy2BkJf1D4KxsGs1Gq9PyAATny4YXlvOLlBBrPR8scM4AcoxW30DckzcMwU--0AliB94FAZMINvx6Arl1YNsUYdmeoflj%7EdLl745tQhzsQXFI7cLF7QuYemJRr0%7ETZvwOr6XGjecJV8C%7EQE4CcJUDsP19zftzCE31FHTJm5WhvGrqGsPbIU4skqQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-22 00:01:52--  https://cdn-lfs-us-1.hf.co/repos/8d/e8/8de8c6dd40cd4e7b492fc1fc1538dafc96a69f82e46d5c29c8f88fe0d8445713/7b99832b3040f184ab3929409de0094bd5edcc485d6343f82bc861fd4c0c308a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%3B+filename%3D%22DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%22%3B&Expires=1740186112&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4NjExMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzhkL2U4LzhkZThjNmRkNDBjZDRlN2I0OTJmYzFmYzE1MzhkYWZjOTZhNjlmODJlNDZkNWMyOWM4Zjg4ZmUwZDg0NDU3MTMvN2I5OTgzMmIzMDQwZjE4NGFiMzkyOTQwOWRlMDA5NGJkNWVkY2M0ODVkNjM0M2Y4MmJjODYxZmQ0YzBjMzA4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fNeoVjZrfaFCG8QJPHbrocJDnuDb92kjFHcJ4vsO6KM-LtTu71BBzN4gJ0fy4Nkm0HsBujxUaseRLn%7EPZqno1%7Ewyxl5bqaUHUEw3GgTuR4eCBxcQSkf-CzkxSNtuqDbkLzryouPg08Xo4oo8pyR343KgESin2zIy2BkJf1D4KxsGs1Gq9PyAATny4YXlvOLlBBrPR8scM4AcoxW30DckzcMwU--0AliB94FAZMINvx6Arl1YNsUYdmeoflj%7EdLl745tQhzsQXFI7cLF7QuYemJRr0%7ETZvwOr6XGjecJV8C%7EQE4CcJUDsP19zftzCE31FHTJm5WhvGrqGsPbIU4skqQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.120, 3.169.36.128, 3.169.36.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 701332064 (669M) [binary/octet-stream]\n",
            "Saving to: ‘DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf’\n",
            "\n",
            "DeepSeek-R1-Distill 100%[===================>] 668.84M  40.9MB/s    in 17s     \n",
            "\n",
            "2025-02-22 00:02:09 (40.4 MB/s) - ‘DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf’ saved [701332064/701332064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9GWZxQJ9a4h",
        "outputId": "3db03cf6-305d-42df-d429-069007b00551"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRAIoiMJ97-S",
        "outputId": "586607a4-ece3-4957-824f-5f7ff0ac430c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip -d /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VprgVg3j-J-x",
        "outputId": "90ac26cd-5f6b-4a6d-88cf-9fff9e54245c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.unpack_archive(\"/content/compressed_folder.zip\", \"/content/extracted_folder\", \"zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Vo3F4-v5-Q3_",
        "outputId": "a076f6f5-6468-4863-ee25-8dd06a5d4529"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ReadError",
          "evalue": "/content/compressed_folder.zip is not a zip file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9760b324f113>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/compressed_folder.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/extracted_folder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36munpack_archive\u001b[0;34m(filename, extract_dir, format, filter)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0;31m# we need to look at the registered unpackers supported extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_unpack_zipfile\u001b[0;34m(filename, extract_dir)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not a zip file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[0mzip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadError\u001b[0m: /content/compressed_folder.zip is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A84aulXa-6xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "079c6OtZ-6uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsrzUNF--6rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip -d /content/extracted_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhmmX59e-6nx",
        "outputId": "9247106a-416f-4ba9-b2b9-00f80fd063ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "   creating: /content/extracted_folder/content/llama.cpp/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/spm-headers/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-cpu.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-cpp.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-metal.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-alloc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-backend.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.ecrc  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_hf_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pyproject.toml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/pyproject.toml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/py.typed  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/constants.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/tensor_mapping.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/utility.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/vocab.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf_reader.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/quants.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/lazy.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf_writer.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_dump.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_convert_endian.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_new_metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_set_metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_hash.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/LICENSE  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/writer.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/reader.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/test_metadata.py  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/gguf-py/tests/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/test_quants.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/arg.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/sampling.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/stb_image.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/build-info.cpp.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/cmake/build-info-gen-cpp.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/common.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json-schema-to-grammar.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/arg.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/minja/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/minja/chat-template.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/minja/minja.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/llguidance.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/chat.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/build-info.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/speculative.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/log.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/base64.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/ngram-cache.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/console.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/console.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/speculative.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/log.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json-schema-to-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/ngram-cache.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.editorconfig  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/models/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-baichuan.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/models/.editorconfig  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-neox.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-aquila.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf.inp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/models/templates/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.3-70B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.1-8B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/Qwen-Qwen2.5-7B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.2-3B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/fireworks-ai-llama-3-firefunction-v2.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/llama-cpp-deepseek-r1.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meetkai-functionary-medium-v3.2.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meetkai-functionary-medium-v3.1.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/mistralai-Mistral-Nemo-Instruct-2407.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/CohereForAI-c4ai-command-r-plus-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/microsoft-Phi-3.5-mini-instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/google-gemma-2-2b-it.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-chameleon.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-chameleon.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CODEOWNERS  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/media/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama1-logo.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/matmul.svg  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/matmul.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama1-banner.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama0-logo.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama0-banner.png  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/llama-config.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/git-vars.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/x64-windows-llvm.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/common.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/build-info.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-windows-msvc.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-windows-llvm.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/llama.pc.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-apple-clang.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/flake.lock  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.gitmodules  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/poetry.lock  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.gitignore  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/bin/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-tts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-backend-ops  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-infill  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-vdot  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-qwen2vl-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-embedding  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-merge  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-llava-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf-split  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libllava_shared.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-chat-template  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-imatrix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-rope  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-simple-chat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-barrier  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-create  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-grammar-integration  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-tokenize  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-model-load-cancel  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-speculative-simple  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-passkey  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-stats  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-chat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-json-schema-to-grammar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-batched-bench  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gritlm  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-quantize-stats  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-cvector-generator  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-parallel  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-bench  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-quantize-fns  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml-cpu.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-perplexity  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-speculative  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-arg-parser  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-minicpmv-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-q8dot  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf-hash  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-server  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gen-docs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookahead  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-eval-callback  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-export-lora  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-log  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-grammar-parser  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-quantize  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-quantize-perf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-0  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-simple  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-1-spm  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-run  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-batched  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-autorelease  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-llama-grammar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libllama.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-save-load-state  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-sampling  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gbnf-validator  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-retrieval  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/TargetDirectories.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Makefile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_C.bin  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_CXX.bin  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeRuleHashes.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/pkgRedirects/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeConfigureLog.yaml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeScratch/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Makefile2  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeDetermineCompilerABI_CXX.bin  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeCCompiler.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/tmp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/CMakeCXXCompilerId.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/a.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeDetermineCompilerABI_C.bin  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/tmp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/CMakeCCompilerId.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/a.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeCXXCompiler.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeSystem.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/cmake.check_cache  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean_target.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/libcommon.a  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/Testing/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/Testing/Temporary/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeCache.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/DartConfiguration.tcl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/compile_commands.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama-version.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean_target.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/libllava_static.a  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/index.html.gz.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/loading.html.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/ggml-config.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/progress.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/ggml-version.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama-config.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama.pc  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_lora_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_llama_ggml_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.clang-format  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.clang-tidy  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.dockerignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Package.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/flake.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/AUTHORS  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/index  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/tags/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/heads/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/refs/heads/master  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/origin/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/origin/HEAD  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/hooks/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/update.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-receive.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-rebase.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-push.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/commit-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/post-update.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-commit.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/fsmonitor-watchman.sample  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/info/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/info/exclude  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/config  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/heads/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/heads/master  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/origin/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/HEAD  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/info/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/pack-b05ec5866c1c8dec8ab0f105994881ece4df5da6.idx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/pack-b05ec5866c1c8dec8ab0f105994881ece4df5da6.pack  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/description  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/packed-refs  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/branches/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/HEAD  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/mypy.ini  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_hf_to_gguf_update.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/labeler.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/pull_request_template.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/workflows/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/build.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/labeler.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/docker.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/editorconfig.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-type-check.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/server.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-lint.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/bench.yml.disabled  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/close-issue.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-check-requirements.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/gguf-publish.yml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/030-research.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/config.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/019-bug-misc.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/010-bug-compilation.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/020-enhancement.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/040-refactor.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/011-bug-results.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/LICENSE  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CMakePresets.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/src/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-grammar.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-batch.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/src/llama-cparams.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-context.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-context.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode-data.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-batch.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-hparams.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-impl.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-grammar.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/src/llama-quant.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-arch.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-cparams.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-hparams.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-adapter.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model-loader.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-mmap.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-vocab.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-sampling.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-kv-cache.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model-loader.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-kv-cache.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-chat.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-adapter.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-impl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode-data.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-arch.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-vocab.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-quant.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-mmap.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ci/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ci/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ci/run.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/tokenize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tokenize/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tokenize/tokenize.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/imatrix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/imatrix.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-13B.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/completions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/cvector-generator.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/positive.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/pca.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/negative.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/mean.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/json_schema_pydantic_example.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/pydantic_models_to_grammar_examples.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/clib.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.c  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/rotate-bits.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/package.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/gguf-hash.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Package.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Sources/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Sources/main.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llava/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-glmedge.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llava/android/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/android/adb_run.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/android/build_64.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmv2.5.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-quantize.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/convert_image_encoder_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/qwen2vl-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip-quantize-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/glmedge-convert-image-encoder-to-gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava_surgery_v2.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/glmedge-surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/qwen2_vl_surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmv2.6.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmo2.6.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-convert-image-encoder-to-gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava_surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/MobileVLM-README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/reason-act.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/ts-type-to-grammar.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/llama_swiftuiApp.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/.gitignore  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/Contents.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/Contents.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/LoadCustomButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ContentView.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/InputButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/LibLlama.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/contents.xcworkspacedata  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/run/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/run.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/linenoise.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/LICENSE  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/linenoise.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/simple.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server_embd.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server-llama2-13B.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/regex_to_grammar.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gritlm/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/gritlm.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/lookup/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-stats.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-create.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-merge.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/speculative-simple.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/batched.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/passkey/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/passkey.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/eval-callback.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/export-lora/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/export-lora.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/tts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/convert_pt_to_hf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/tts-outetts.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/tts.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/gguf-split.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/pydantic_models_to_grammar.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf/gguf.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/parallel/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/parallel.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/llama-bench.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/json_schema_to_grammar.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/sycl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/win-build-sycl.bat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/build.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/win-run-llama2.bat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/ls-sycl-device.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/run-llama2.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/gbnf-validator.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle.properties  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradlew  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/.gitignore  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/consumer-rules.pro  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/proguard-rules.pro  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/AndroidManifest.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/llama-android.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/build.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.jar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.properties  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/build.gradle.kts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/settings.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/proguard-rules.pro  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/AndroidManifest.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Color.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Type.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Theme.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/Downloadable.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainActivity.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_background.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_foreground.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/colors.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/strings.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/themes.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher_round.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/backup_rules.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/data_extraction_rules.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher.webp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/build.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/quantize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/quantize.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/batched-bench.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/httplib.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat_screens.webp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/ui.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/readme.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/datautils.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat.mjs  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/system-prompts.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-mangotango.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index-new.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-ketivah.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-snowstorm.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/style.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-playground.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/completion.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index.js  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/favicon.ico  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-polarnight.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/loading.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-beeninorder.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/colorthemes.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/json-schema-to-grammar.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/prompt-formats.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat-llama2.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/server.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/utils.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/eslint.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/.prettierignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.app.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tailwind.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/package-lock.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/ChatMessage.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/MarkdownDisplay.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/Header.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/ChatScreen.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/CanvasPyInterpreter.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/Sidebar.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/SettingDialog.tsx  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/misc.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/llama-vscode.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/storage.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/common.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/types.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/app.context.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/App.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/index.scss  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/Config.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/main.tsx  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/vite-env.d.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/postcss.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.node.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/vite.config.ts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/public/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/public/demo-conversation.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/bench.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/prometheus.yml  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/bench/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/script.js  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/llama_cpp.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/wild.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/llamapattern.png  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/favicon.ico  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/buttons_top.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/index.html  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/favicon.ico  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public/index.html.gz  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public/loading.html  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/tests/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/tests/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/conftest.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/pytest.ini  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_rerank.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_embedding.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_tokenize.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_completion.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_ctx_shift.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_tool_call.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_lora.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_basic.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_infill.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_chat_completion.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_speculative.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_slot_save.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_security.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/utils.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/Miku.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/retrieval/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/retrieval.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/quantize-stats.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-13B.bat  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/infill/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/infill.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert_legacy_llama.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/speculative/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/speculative.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/main.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.vim  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/lookahead/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/lookahead.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/save-load-state.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/rpc/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/rpc-server.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/jeopardy.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/qasheet.csv  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/graph.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-vicuna.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/embedding/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/embedding.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/simple-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/deprecation-warning.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/gen-docs.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-persistent.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/perplexity/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/perplexity.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llm.vim  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/requirements/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-compare-llama-bench.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-pydantic.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_lora_to_gguf.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_llama_ggml_to_gguf.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-test-tokenizer-random.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_hf_to_gguf_update.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-all.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_legacy_llama.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CONTRIBUTING.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pyrightconfig.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/development/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/debugging-tests.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/HOWTO-add-model.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/idea-arch.key  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/idea-arch.pdf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/token_generation_performance_tips.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/backend/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/SYCL.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/CANN.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/OPENCL.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/BLIS.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/install.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/docker.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/build.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/llguidance.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/cuda-fedora.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/android.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/cmake/ggml-config.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-hip/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-hip/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/ggml-kompute.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_softmax.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n_pre.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_diagmask.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/common.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_gelu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_silu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rmsnorm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale_8.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_addrow.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_mat_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_add.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q8_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/rope_common.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opt.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-alloc.c  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/embed_kernel.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_general.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/ggml-opencl.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/Doxyfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/.clang-format  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/acl_tensor.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/ascendc_kernels.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/acl_tensor.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/ggml-cann.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-quants.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-threading.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/cmake/host-toolchain.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_from_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/group_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/diag_mask_inf.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_m.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pool2d.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/div.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/acc.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_xs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q8_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q3_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/contig_copy.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_vision.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/count_equal.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/silu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/wkv6.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/im2col.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pad.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sub.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/cos.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/soft_max.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/flash_attn_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_xxs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat_back.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/leaky_relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_nl.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xxs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/concat.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat_support.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q3_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argmax.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/timestep_embedding.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu_quick.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argsort.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sin.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/tanh.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_multi.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_split_k_reduce.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/types.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_unary_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/scale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sum_rows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_to_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat2_support.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/opt_step_adamw.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_m.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/square.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/add.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/clamp.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/ggml-vulkan.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend-reg.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal.m  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal-impl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal.metal  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-musa/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-musa/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dequantize.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/ggml-sycl.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gemm.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/common.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/presets.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/concat.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/common.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/softmax.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/convert.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/im2col.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/norm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/vecdotq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmvq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/outprod.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/tsembd.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dmmv.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/concat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/element_wise.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/norm.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/conv.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/im2col.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/conv.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/wkv6.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gla.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/wkv6.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/tsembd.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gla.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dpct/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dpct/helper.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/backend.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/element_wise.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/outprod.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dmmv.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/softmax.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/convert.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/rope.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmvq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/rope.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/dequantize.cuh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/musa.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/cuda.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/hip.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sum.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/norm.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmv.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/count-equal.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/binbcast.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vecdotq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pad.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cp-async.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/wkv6.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/arange.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/softmax.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/wkv6.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/out-prod.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/acc.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/scale.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/clamp.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sumrows.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmvq.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pad.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/quantize.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/diagmask.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argmax.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/getrows.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/count-equal.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f32.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/binbcast.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/rope.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sumrows.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/im2col.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/upscale.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/rope.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/convert.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sum.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cpy.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/out-prod.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pool2d.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/concat.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/concat.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/quantize.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/upscale.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cpy.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-mma-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/diagmask.cu  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb32.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb8.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb64.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/generate_cu_files.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/softmax.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmvq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmv.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/getrows.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/norm.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/im2col.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pool2d.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argsort.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argsort.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/convert.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/tsembd.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/common.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/arange.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/clamp.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/tsembd.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/scale.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mma.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/gla.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmq.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/gla.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/unary.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-common.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/unary.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/acc.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argmax.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/amx.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cmake/FindSIMD.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cpu-feats-x86.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-threading.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/gguf.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-quants.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/ggml-blas.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/include/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-opencl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-sycl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-kompute.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-blas.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cpu.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-opt.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cpp.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-metal.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-rpc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/gguf.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-vulkan.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-alloc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-backend.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cuda.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cann.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.devops/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/intel.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/vulkan.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/musa.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cuda.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/rocm.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cpp.srpm.spec  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cloud-v-pipeline  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.devops/nix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/scope.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/jetson-support.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/sif.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/python-scripts.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/package.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/apps.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/nixpkgs-instances.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/docker.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/devshells.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/package-gguf-py.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cpu.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cli-cann.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cpp-cuda.srpm.spec  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/tools.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/SECURITY.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/include/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/include/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/include/llama-cpp.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/scripts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml-am.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/gen-unicode-data.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/xxd.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml.last  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-flags.mk  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/run-all-perf.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/debug-test.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-wikitext-2.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get_chat_template.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-wikitext-103.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/fetch_server_test_models.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/verify-checksum-models.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/build-info.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/run-all-ppl.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/check-requirements.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/ci-run.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-hellaswag.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/gen-authors.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/hf.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/qnt-all.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/compare-llama-bench.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-winogrande.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-pg.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/compare-commits.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/install-oneapi.bat  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/pocs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/pocs/vdot/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/q8dot.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/vdot.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/prompts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/dan.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-bob.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-baichuan.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/alpaca.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-vicuna-v1.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/reason-act.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/LLM-questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-vicuna-v0.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/assistant.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/prompts/chat-with-qwen.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/parallel-questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/dan-modified.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/mnemonics.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.pre-commit-config.yaml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/Sources/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/Sources/llama/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Sources/llama/module.modulemap  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/Sources/llama/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.flake8  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-llguidance.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-double-float.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/run-json-schema-to-grammar.mjs  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/tests/get-model.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-1-spm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-backend-ops.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-arg-parser.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-log.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-quantize-perf.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/tests/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-integration.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-random.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-opt.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-autorelease.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-llama-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-quantize-fns.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-barrier.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-rope.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-1-bpe.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-chat-template.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-parser.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-lora-conversion-inference.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-gguf.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/get-model.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-c.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-json-schema-to-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-model-load-cancel.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/grammars/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/c.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/json_arr.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/chess.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/json.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/english.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/list.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/japanese.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/arithmetic.gbnf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWvPWg1vAsnQ",
        "outputId": "c815f832-f3b7-46e8-80d6-2fac66b5bf1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kQIxMHvAuM4",
        "outputId": "c2b876b6-3bec-48e0-c362-75fe1abc1bfa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/extracted_folder/content/llama.cpp/build/bin\n"
      ],
      "metadata": {
        "id": "lXBnYfCSAzUA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnwIhz7lCIqZ",
        "outputId": "56c681be-ab3b-4922-a2e9-c54aa1765716"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/extracted_folder/content/llama.cpp/build/bin/libllama.so /usr/lib/\n",
        "!ldconfig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY8nvBmZCKCB",
        "outputId": "4592e6d8-a7b5-4b8b-81f7-f84107314286"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libllama.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PukY--w1CTbo",
        "outputId": "cd19b6f4-745b-40e7-dbd9-7735773a4c8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/extracted_folder -name \"libggml.so\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF6eIhTACZqY",
        "outputId": "44634378-120c-4fa9-c21a-621e76e9bbc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHP0RJ1rCp6A",
        "outputId": "8e31e042-3dd2-46c3-f3bb-5d3adb10ea21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007ffc12385000)\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007d7f89fb9000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007d7f89f99000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007d7f89d70000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007d7f89c87000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007d7f8a20c000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/extracted_folder/content/llama.cpp/build/bin\n"
      ],
      "metadata": {
        "id": "NTKTw7ZAC8WI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902VwWcLCqeh",
        "outputId": "1a7901e8-37e2-411f-96c2-864e6a26c05d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsrqk0GDFDp",
        "outputId": "735fa804-9618-497d-b73f-3c39bdd9b760"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007fff35ddc000)\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ef846b4b000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ef846b2b000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ef846902000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ef846819000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007ef846d9e000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/extracted_folder/content/llama.cpp/build/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe_we9hgEoNi",
        "outputId": "16df2721-ceec-42e7-fb87-cfc7a33a9e0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-VZmWDmErf6",
        "outputId": "e6b493a2-0d67-442a-8d1c-c7a4e0c67c22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libggml-base.so\t\t       llama-llava-clip-quantize-cli  llama-tts\n",
            "libggml-cpu.so\t\t       llama-lookahead\t\t      llama-vdot\n",
            "libggml.so\t\t       llama-lookup\t\t      test-arg-parser\n",
            "libllama.so\t\t       llama-lookup-create\t      test-autorelease\n",
            "libllava_shared.so\t       llama-lookup-merge\t      test-backend-ops\n",
            "llama-batched\t\t       llama-lookup-stats\t      test-barrier\n",
            "llama-batched-bench\t       llama-minicpmv-cli\t      test-c\n",
            "llama-bench\t\t       llama-parallel\t\t      test-chat\n",
            "llama-cli\t\t       llama-passkey\t\t      test-chat-template\n",
            "llama-convert-llama2c-to-ggml  llama-perplexity\t\t      test-gguf\n",
            "llama-cvector-generator        llama-q8dot\t\t      test-grammar-integration\n",
            "llama-embedding\t\t       llama-quantize\t\t      test-grammar-parser\n",
            "llama-eval-callback\t       llama-quantize-stats\t      test-json-schema-to-grammar\n",
            "llama-export-lora\t       llama-qwen2vl-cli\t      test-llama-grammar\n",
            "llama-gbnf-validator\t       llama-retrieval\t\t      test-log\n",
            "llama-gen-docs\t\t       llama-run\t\t      test-model-load-cancel\n",
            "llama-gguf\t\t       llama-save-load-state\t      test-quantize-fns\n",
            "llama-gguf-hash\t\t       llama-server\t\t      test-quantize-perf\n",
            "llama-gguf-split\t       llama-simple\t\t      test-rope\n",
            "llama-gritlm\t\t       llama-simple-chat\t      test-sampling\n",
            "llama-imatrix\t\t       llama-speculative\t      test-tokenizer-0\n",
            "llama-infill\t\t       llama-speculative-simple       test-tokenizer-1-bpe\n",
            "llama-llava-cli\t\t       llama-tokenize\t\t      test-tokenizer-1-spm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/extracted_folder/content/llama.cpp/build/bin/libggml.so /usr/lib/\n",
        "!ldconfig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL1cku7HDLuB",
        "outputId": "d536472c-980f-4088-9b96-db79c84d08f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQUQ3Q_EEVSB",
        "outputId": "45a0f814-552d-4668-e8d2-02efc6790be0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTzU7Q2iEck5",
        "outputId": "bc15e008-db30-4df7-8168-7bbbfd434272"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007ffc247ca000)\n",
            "\tlibllama.so => /lib/libllama.so (0x00007c656869d000)\n",
            "\tlibggml.so => /lib/libggml.so (0x00007c656868e000)\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007c6568462000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007c656837b000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007c6568359000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007c6568130000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007c656899f000)\n",
            "\tlibggml-base.so => not found\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aشغال"
      ],
      "metadata": {
        "id": "o5dXyO-sE9IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiOqHQP5Eh4g",
        "outputId": "c9fb7f66-dcb5-4cb6-9e62-a73b7f2ada9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default: edkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               reasoning format (default: deepseek; allowed values: deepseek, none)\n",
            "                                        controls whether thought tags are extracted from the response, and in\n",
            "                                        which format they're returned. 'none' leaves thoughts unparsed in\n",
            "                                        `message.content`, 'deepseek' puts them in `message.reasoning_content`\n",
            "                                        (for DeepSeek R1 & Command R7B only).\n",
            "                                        only supported for non-streamed responses\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,\n",
            "                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,\n",
            "                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,\n",
            "                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,\n",
            "                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,\n",
            "                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,\n",
            "                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "SZXq1LyJGA0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KCrlsUOE5a5",
        "outputId": "33b4a5eb-8d3e-4981-b492-9b0a63169dec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4753 (51f311e0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 339 tensors from /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                          general.file_type u32              = 29\n",
            "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models_out/DeepSeek-R1-Distill-Qwen-...\n",
            "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 128\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:   28 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq3_s:   32 tensors\n",
            "llama_model_loader: - type iq2_s:  137 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = IQ2_M - 2.7 bpw\n",
            "print_info: file size   = 663.17 MiB (3.13 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.78 B\n",
            "print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size =   663.17 MiB\n",
            ".................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 2048\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
            "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   299.75 MiB\n",
            "llama_init_from_model: graph nodes  = 986\n",
            "llama_init_from_model: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "main: chat template example:\n",
            "You are a helpful assistant\n",
            "\n",
            "<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 4279003682\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Using default system message. To change it, set a different value via -p PROMPT or -f FILE argument.\n",
            "\n",
            "You are a helpful assistant\n",
            "\n",
            "\n",
            "> \n",
            "llama_perf_sampler_print:    sampling time =       0.01 ms /     7 runs   (    0.00 ms per token, 636363.64 tokens per second)\n",
            "llama_perf_context_print:        load time =    3554.26 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   63121.31 ms /     2 tokens\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "_5CMCxFUF90E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39DejDWIFYV6",
        "outputId": "99348995-817c-4933-8a05-b12286649b2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4753 (51f311e0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 339 tensors from /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151646\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                          general.file_type u32              = 29\n",
            "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models_out/DeepSeek-R1-Distill-Qwen-...\n",
            "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 128\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:   28 tensors\n",
            "llama_model_loader: - type q5_K:    1 tensors\n",
            "llama_model_loader: - type iq3_s:   32 tensors\n",
            "llama_model_loader: - type iq2_s:  137 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = IQ2_M - 2.7 bpw\n",
            "print_info: file size   = 663.17 MiB (3.13 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.78 B\n",
            "print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size =   663.17 MiB\n",
            ".................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 2048\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
            "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   299.75 MiB\n",
            "llama_init_from_model: graph nodes  = 986\n",
            "llama_init_from_model: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "main: chat template example:\n",
            "You are a helpful assistant\n",
            "\n",
            "<｜User｜>Hello<｜Assistant｜>Hi there<｜end▁of▁sentence｜><｜User｜>How are you?<｜Assistant｜>\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 1414910009\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            "\n",
            "Hello, how are you?\n",
            "\n",
            "\n",
            "> hi\n",
            "<think>\n",
            "Okay, so I've been told to respond to a message that says, \"Hello, how are you?\" and then follows up with \"Hi!\" and \"Thanks for that!\". Hmm, I'm not entirely sure how to approach this. Maybe I should start by understanding why someone would ask this question. It could be to welcome back someone, offer assistance, or just keep the conversation going.\n",
            "\n",
            "I think a good response should be friendly and welcoming. Maybe something like, \"Hello! I'm glad to hear you're doing well. How can I assist you today? If you have any questions, feel free to ask!\"\n",
            "\n",
            "Wait, but should I mention how I'm doing? Maybe start with a greeting and then mention my current state. That might make it more personal.\n",
            "\n",
            "Alternatively, I could say, \"Hi! How are you? Feel free to ask if you need help. Thanks for your interest!\"\n",
            "\n",
            "I think starting with a greeting is important because it introduces the reader or the person who's responding. Then, perhaps offering assistance would be good. I should\n",
            "llama_perf_sampler_print:    sampling time =      41.94 ms /   221 runs   (    0.19 ms per token,  5269.56 tokens per second)\n",
            "llama_perf_context_print:        load time =    1178.11 ms\n",
            "llama_perf_context_print: prompt eval time =    7921.40 ms /    12 tokens (  660.12 ms per token,     1.51 tokens per second)\n",
            "llama_perf_context_print:        eval time =   55201.78 ms /   215 runs   (  256.75 ms per token,     3.89 tokens per second)\n",
            "llama_perf_context_print:       total time =   63499.04 ms /   227 tokens\n",
            "Interrupted by user\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ce8738f83594779be525e3c17ef92d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ceddb62e0a034776ac9dbcfd66cbebae"
          }
        },
        "49cb48a3b3cb446d9f587cf9c2563049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588e41f5b6d44f7997af1f3ed46759a7",
            "placeholder": "​",
            "style": "IPY_MODEL_7df4f1a8a4a8495f9482fadd32593c83",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0b501236cc054d719eda31c7ed9c4dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_af22f3975fbd4f0dbef8d1bd6671f90c",
            "placeholder": "​",
            "style": "IPY_MODEL_8e2678c1b1194d6d84ea56254bd2bf50",
            "value": ""
          }
        },
        "2f8fd749746245c6a4c8fa654fc95f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0fe01a16de6d4659967d5f9297042823",
            "style": "IPY_MODEL_4f5f291019c349d5952f01358aa7ddf7",
            "value": true
          }
        },
        "c8aa7649366b447680548b78bc42a25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e2f7a005469d4647999b5b4b176e1725",
            "style": "IPY_MODEL_9394e03a32ba497c9dbb4fecc93381b0",
            "tooltip": ""
          }
        },
        "aee6ff4d588b4025a8d15f89243b3fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_797cf05599464d1984e2d1fc5e6b6e1b",
            "placeholder": "​",
            "style": "IPY_MODEL_20d6638307f54869a1d7ad6444c5887c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ceddb62e0a034776ac9dbcfd66cbebae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "588e41f5b6d44f7997af1f3ed46759a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df4f1a8a4a8495f9482fadd32593c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af22f3975fbd4f0dbef8d1bd6671f90c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2678c1b1194d6d84ea56254bd2bf50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe01a16de6d4659967d5f9297042823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f5f291019c349d5952f01358aa7ddf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2f7a005469d4647999b5b4b176e1725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9394e03a32ba497c9dbb4fecc93381b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "797cf05599464d1984e2d1fc5e6b6e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d6638307f54869a1d7ad6444c5887c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "316cdf14f3e14aa2bb3eed1fc4601ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d45f3d53ef84936bba42f4ddbe3ccdf",
            "placeholder": "​",
            "style": "IPY_MODEL_afade67647544d85b7e481df1143b84d",
            "value": "Connecting..."
          }
        },
        "9d45f3d53ef84936bba42f4ddbe3ccdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afade67647544d85b7e481df1143b84d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c465317f8fef46fba531cacd3778305d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_521f0658944f4633849915e33259c67d",
              "IPY_MODEL_935cc44b7d2a4081acded1bbba868a4d",
              "IPY_MODEL_1d6233c9aea8427a9dca6feb5743c423"
            ],
            "layout": "IPY_MODEL_1efe15bce10241aa9effb8543b4a989e"
          }
        },
        "521f0658944f4633849915e33259c67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ac09e5ba744679835742767983254e",
            "placeholder": "​",
            "style": "IPY_MODEL_69d9af1f54b24992a1235eeb28d83beb",
            "value": "llama-v2-70b-q2k.gguf: "
          }
        },
        "935cc44b7d2a4081acded1bbba868a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eaba38008fb4889b0f822b15537f44d",
            "max": 22924882976,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94582946ef3e404cafd6a29b5aa265ce",
            "value": 22924882976
          }
        },
        "1d6233c9aea8427a9dca6feb5743c423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ab45aad7f94487940e2784e3aea820",
            "placeholder": "​",
            "style": "IPY_MODEL_be8f1274bc7244ff9e212aa8fecdbbbe",
            "value": " 22.9G/? [12:23&lt;00:00, 35.3MB/s]"
          }
        },
        "1efe15bce10241aa9effb8543b4a989e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ac09e5ba744679835742767983254e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d9af1f54b24992a1235eeb28d83beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eaba38008fb4889b0f822b15537f44d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94582946ef3e404cafd6a29b5aa265ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36ab45aad7f94487940e2784e3aea820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be8f1274bc7244ff9e212aa8fecdbbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7a02f37056c479f9395c8e1dd742d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca5bd697c60c4f2e977322a25953e532",
              "IPY_MODEL_e08f326ec9214491bfd88ba253b915bd",
              "IPY_MODEL_36d2ac93edc64366a6af890aa2736749"
            ],
            "layout": "IPY_MODEL_42cc56b2ed674420bd3dfe5dafc22030"
          }
        },
        "ca5bd697c60c4f2e977322a25953e532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3bf31db040a420a948f82ba5a2826c9",
            "placeholder": "​",
            "style": "IPY_MODEL_34151d301d30445983cbb173578b9554",
            "value": "llama-v2-7b-q2k.gguf: 100%"
          }
        },
        "e08f326ec9214491bfd88ba253b915bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f92940604614bb4ac230efcbf5d443f",
            "max": 2277273152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2cce4656a93455bb34ea33d3149f129",
            "value": 2277273152
          }
        },
        "36d2ac93edc64366a6af890aa2736749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba0820739c4f4d9981f32a5d2bfcba16",
            "placeholder": "​",
            "style": "IPY_MODEL_40ca005eb9d745468fb062a9078d43fc",
            "value": " 2.28G/2.28G [01:32&lt;00:00, 29.2MB/s]"
          }
        },
        "42cc56b2ed674420bd3dfe5dafc22030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3bf31db040a420a948f82ba5a2826c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34151d301d30445983cbb173578b9554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f92940604614bb4ac230efcbf5d443f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2cce4656a93455bb34ea33d3149f129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba0820739c4f4d9981f32a5d2bfcba16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ca005eb9d745468fb062a9078d43fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSCIXi4v2Tow"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxR36fPS9ZXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "تم تشغيل لاما من مجلد تم ضغطه بعد البناء وتحميله ولم يعمل الا من خلالا الامر\n",
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HU9f7XKBHdk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "id": "1foSlv04HiRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoNQivvp9Zac",
        "outputId": "3f970c7f-f090-4abc-f592-b5ced7ef106d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-22 00:01:51--  https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.19, 3.169.137.5, 3.169.137.119, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/8d/e8/8de8c6dd40cd4e7b492fc1fc1538dafc96a69f82e46d5c29c8f88fe0d8445713/7b99832b3040f184ab3929409de0094bd5edcc485d6343f82bc861fd4c0c308a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%3B+filename%3D%22DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%22%3B&Expires=1740186112&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4NjExMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzhkL2U4LzhkZThjNmRkNDBjZDRlN2I0OTJmYzFmYzE1MzhkYWZjOTZhNjlmODJlNDZkNWMyOWM4Zjg4ZmUwZDg0NDU3MTMvN2I5OTgzMmIzMDQwZjE4NGFiMzkyOTQwOWRlMDA5NGJkNWVkY2M0ODVkNjM0M2Y4MmJjODYxZmQ0YzBjMzA4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fNeoVjZrfaFCG8QJPHbrocJDnuDb92kjFHcJ4vsO6KM-LtTu71BBzN4gJ0fy4Nkm0HsBujxUaseRLn%7EPZqno1%7Ewyxl5bqaUHUEw3GgTuR4eCBxcQSkf-CzkxSNtuqDbkLzryouPg08Xo4oo8pyR343KgESin2zIy2BkJf1D4KxsGs1Gq9PyAATny4YXlvOLlBBrPR8scM4AcoxW30DckzcMwU--0AliB94FAZMINvx6Arl1YNsUYdmeoflj%7EdLl745tQhzsQXFI7cLF7QuYemJRr0%7ETZvwOr6XGjecJV8C%7EQE4CcJUDsP19zftzCE31FHTJm5WhvGrqGsPbIU4skqQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-22 00:01:52--  https://cdn-lfs-us-1.hf.co/repos/8d/e8/8de8c6dd40cd4e7b492fc1fc1538dafc96a69f82e46d5c29c8f88fe0d8445713/7b99832b3040f184ab3929409de0094bd5edcc485d6343f82bc861fd4c0c308a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%3B+filename%3D%22DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf%22%3B&Expires=1740186112&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4NjExMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzhkL2U4LzhkZThjNmRkNDBjZDRlN2I0OTJmYzFmYzE1MzhkYWZjOTZhNjlmODJlNDZkNWMyOWM4Zjg4ZmUwZDg0NDU3MTMvN2I5OTgzMmIzMDQwZjE4NGFiMzkyOTQwOWRlMDA5NGJkNWVkY2M0ODVkNjM0M2Y4MmJjODYxZmQ0YzBjMzA4YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=fNeoVjZrfaFCG8QJPHbrocJDnuDb92kjFHcJ4vsO6KM-LtTu71BBzN4gJ0fy4Nkm0HsBujxUaseRLn%7EPZqno1%7Ewyxl5bqaUHUEw3GgTuR4eCBxcQSkf-CzkxSNtuqDbkLzryouPg08Xo4oo8pyR343KgESin2zIy2BkJf1D4KxsGs1Gq9PyAATny4YXlvOLlBBrPR8scM4AcoxW30DckzcMwU--0AliB94FAZMINvx6Arl1YNsUYdmeoflj%7EdLl745tQhzsQXFI7cLF7QuYemJRr0%7ETZvwOr6XGjecJV8C%7EQE4CcJUDsP19zftzCE31FHTJm5WhvGrqGsPbIU4skqQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.120, 3.169.36.128, 3.169.36.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 701332064 (669M) [binary/octet-stream]\n",
            "Saving to: ‘DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf’\n",
            "\n",
            "DeepSeek-R1-Distill 100%[===================>] 668.84M  40.9MB/s    in 17s     \n",
            "\n",
            "2025-02-22 00:02:09 (40.4 MB/s) - ‘DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf’ saved [701332064/701332064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9GWZxQJ9a4h",
        "outputId": "3db03cf6-305d-42df-d429-069007b00551"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRAIoiMJ97-S",
        "outputId": "586607a4-ece3-4957-824f-5f7ff0ac430c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip -d /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VprgVg3j-J-x",
        "outputId": "90ac26cd-5f6b-4a6d-88cf-9fff9e54245c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/compressed_folder.zip or\n",
            "        /content/compressed_folder.zip.zip, and cannot find /content/compressed_folder.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.unpack_archive(\"/content/compressed_folder.zip\", \"/content/extracted_folder\", \"zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Vo3F4-v5-Q3_",
        "outputId": "a076f6f5-6468-4863-ee25-8dd06a5d4529"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ReadError",
          "evalue": "/content/compressed_folder.zip is not a zip file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9760b324f113>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/compressed_folder.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/extracted_folder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36munpack_archive\u001b[0;34m(filename, extract_dir, format, filter)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0;31m# we need to look at the registered unpackers supported extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_unpack_zipfile\u001b[0;34m(filename, extract_dir)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not a zip file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[0mzip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadError\u001b[0m: /content/compressed_folder.zip is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A84aulXa-6xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "079c6OtZ-6uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/extracted_folder"
      ],
      "metadata": {
        "id": "lsrzUNF--6rX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/compressed_folder.zip -d /content/extracted_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhmmX59e-6nx",
        "outputId": "7eedb445-9f7f-4c8e-8f82-33a926da2e5c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/compressed_folder.zip\n",
            "   creating: /content/extracted_folder/content/llama.cpp/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/spm-headers/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-cpu.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-cpp.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-metal.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-alloc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/spm-headers/ggml-backend.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.ecrc  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_hf_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pyproject.toml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/pyproject.toml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/py.typed  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/constants.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/tensor_mapping.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/utility.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/vocab.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf_reader.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/quants.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/lazy.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/gguf_writer.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_dump.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_convert_endian.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_new_metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_set_metadata.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/gguf/scripts/gguf_hash.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/LICENSE  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/writer.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/examples/reader.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/test_metadata.py  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/gguf-py/tests/__init__.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/gguf-py/tests/test_quants.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/arg.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/sampling.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/stb_image.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/build-info.cpp.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/cmake/build-info-gen-cpp.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/common.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json-schema-to-grammar.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/arg.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/common/minja/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/minja/chat-template.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/minja/minja.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/llguidance.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/chat.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/build-info.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/speculative.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/log.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/base64.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/ngram-cache.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/console.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/console.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/speculative.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/log.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/json-schema-to-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/common/ngram-cache.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.editorconfig  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/models/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-baichuan.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/models/.editorconfig  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-neox.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-refact.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-aquila.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-qwen2.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-command-r.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-bpe.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-gpt-2.gguf.inp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/models/templates/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.3-70B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.1-8B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/Qwen-Qwen2.5-7B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meta-llama-Llama-3.2-3B-Instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/fireworks-ai-llama-3-firefunction-v2.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/llama-cpp-deepseek-r1.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meetkai-functionary-medium-v3.2.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/meetkai-functionary-medium-v3.1.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/mistralai-Mistral-Nemo-Instruct-2407.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/CohereForAI-c4ai-command-r-plus-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/microsoft-Phi-3.5-mini-instruct.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/templates/google-gemma-2-2b-it.jinja  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-starcoder.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-falcon.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-llama-spm.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-bert-bge.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-mpt.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-chameleon.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-phi-3.gguf.inp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/models/ggml-vocab-chameleon.gguf.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CODEOWNERS  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/media/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama1-logo.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/matmul.svg  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/matmul.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama1-banner.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama0-logo.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/media/llama0-banner.png  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/llama-config.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/git-vars.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/x64-windows-llvm.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/common.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/build-info.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-windows-msvc.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-windows-llvm.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/llama.pc.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/cmake/arm64-apple-clang.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/flake.lock  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.gitmodules  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/poetry.lock  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.gitignore  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/bin/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-tts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-backend-ops  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-infill  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-vdot  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-qwen2vl-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-embedding  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-merge  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-llava-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf-split  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libllava_shared.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-chat-template  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-imatrix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-rope  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-simple-chat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-barrier  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-create  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-grammar-integration  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-tokenize  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-model-load-cancel  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-speculative-simple  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-passkey  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookup-stats  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-chat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-json-schema-to-grammar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-batched-bench  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gritlm  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-quantize-stats  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-cvector-generator  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-parallel  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-bench  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-quantize-fns  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libggml-cpu.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-perplexity  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-speculative  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-arg-parser  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-minicpmv-cli  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-q8dot  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf-hash  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-server  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gen-docs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-lookahead  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-eval-callback  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-export-lora  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-log  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-grammar-parser  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-quantize  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-quantize-perf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-0  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-simple  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-tokenizer-1-spm  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-run  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-batched  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gguf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-autorelease  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-llama-grammar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/libllama.so  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-save-load-state  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/test-sampling  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-gbnf-validator  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/bin/llama-retrieval  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/TargetDirectories.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Makefile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_C.bin  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_CXX.bin  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Experimental.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeRuleHashes.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Continuous.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/pkgRedirects/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeConfigureLog.yaml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/CMakeScratch/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Makefile2  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeDetermineCompilerABI_CXX.bin  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeCCompiler.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/tmp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/CMakeCXXCompilerId.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdCXX/a.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeDetermineCompilerABI_C.bin  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/tmp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/CMakeCCompilerId.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CompilerIdC/a.out  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeCXXCompiler.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/3.31.4/CMakeSystem.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/Nightly.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/cmake.check_cache  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/NightlyTest.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/build_info.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean_target.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/common/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/libcommon.a  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/common/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/Testing/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/Testing/Temporary/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/CMakeCache.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/DartConfiguration.tcl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/compile_commands.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama-version.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/src/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/src/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tokenize/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/imatrix/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cvector-generator/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-hash/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.ts  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean_target.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llava/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/libllava_static.a  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llava/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/convert-llama2c-to-ggml/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/run/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/run/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/simple/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gritlm/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/lookup/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookup/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative-simple/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/batched/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/passkey/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/passkey/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/eval-callback/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/export-lora/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/tts/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/tts/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf-split/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gguf/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gguf/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/parallel/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/parallel/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/llama-bench/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gbnf-validator/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/quantize/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/batched-bench/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/server/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/index.html.gz.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/loading.html.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/server/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/retrieval/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/quantize-stats/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/infill/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/infill/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/speculative/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/speculative/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/main/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/main/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/lookahead/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/save-load-state/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/embedding/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/simple-chat/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/gen-docs/cmake_install.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/perplexity/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/examples/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/ggml-config.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/progress.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/build.make  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/src/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/ggml-cpu/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/src/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/ggml-version.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/ggml/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama-config.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/pocs/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/CMakeFiles/progress.marks  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/vdot/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/pocs/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/llama.pc  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CTestTestfile.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-c.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o.d  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/progress.marks  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o.d  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/DependInfo.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/cmake_clean.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/link.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/progress.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/flags.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/build.make  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o.d  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/tests/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/build/cmake_install.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_lora_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_llama_ggml_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.clang-format  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.clang-tidy  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.dockerignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Package.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/flake.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/AUTHORS  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/index  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/tags/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/heads/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/refs/heads/master  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/origin/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/refs/remotes/origin/HEAD  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/hooks/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/update.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-receive.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-rebase.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-push.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/commit-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/post-update.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-commit.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/hooks/fsmonitor-watchman.sample  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/info/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/info/exclude  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/config  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/heads/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/heads/master  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/origin/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/logs/HEAD  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/info/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/pack-b05ec5866c1c8dec8ab0f105994881ece4df5da6.idx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/objects/pack/pack-b05ec5866c1c8dec8ab0f105994881ece4df5da6.pack  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/description  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.git/packed-refs  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.git/branches/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/.git/HEAD  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/mypy.ini  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/convert_hf_to_gguf_update.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/labeler.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/pull_request_template.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/workflows/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/build.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/labeler.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/docker.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/editorconfig.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-type-check.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/server.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-lint.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/bench.yml.disabled  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/close-issue.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/python-check-requirements.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/workflows/gguf-publish.yml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/030-research.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/config.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/019-bug-misc.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/010-bug-compilation.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/020-enhancement.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/040-refactor.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.github/ISSUE_TEMPLATE/011-bug-results.yml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/LICENSE  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CMakePresets.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/src/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-grammar.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-batch.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/src/llama-cparams.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-context.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-context.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode-data.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-batch.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-hparams.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-impl.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-grammar.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/src/llama-quant.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-arch.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-cparams.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-hparams.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-adapter.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model-loader.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-mmap.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-vocab.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-sampling.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-kv-cache.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model-loader.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-model.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-kv-cache.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-chat.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-adapter.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-impl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/unicode-data.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-arch.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-vocab.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-quant.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/src/llama-mmap.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ci/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ci/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ci/run.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/tokenize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tokenize/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tokenize/tokenize.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/imatrix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/imatrix.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/imatrix/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-13B.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/completions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/cvector-generator.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/positive.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/pca.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/negative.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/cvector-generator/mean.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/json_schema_pydantic_example.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/pydantic_models_to_grammar_examples.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha256/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/clib.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.c  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/rotate-bits.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/rotate-bits/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/deps/sha1/package.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-hash/gguf-hash.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Package.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Makefile  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Sources/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched.swift/Sources/main.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llava/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-glmedge.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llava/android/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/android/adb_run.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/android/build_64.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmv2.5.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-quantize.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/convert_image_encoder_to_gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/qwen2vl-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip-quantize-cli.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/glmedge-convert-image-encoder-to-gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava_surgery_v2.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/clip.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/glmedge-surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/qwen2_vl_surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmv2.6.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/README-minicpmo2.6.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/minicpmv-convert-image-encoder-to-gguf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/llava_surgery.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llava/MobileVLM-README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/reason-act.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/ts-type-to-grammar.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert-llama2c-to-ggml/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/llama_swiftuiApp.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/.gitignore  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/Contents.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/Contents.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/LoadCustomButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ContentView.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/InputButton.swift  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/LibLlama.swift  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/contents.xcworkspacedata  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/run/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/run.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/linenoise.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/LICENSE  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/run/linenoise.cpp/linenoise.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/simple.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server_embd.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server-llama2-13B.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/regex_to_grammar.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gritlm/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gritlm/gritlm.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/lookup/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-stats.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-create.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup-merge.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookup/lookup.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/speculative-simple.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative-simple/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched/batched.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/passkey/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/passkey.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/passkey/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/eval-callback/eval-callback.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/export-lora/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/export-lora.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/export-lora/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/tts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/convert_pt_to_hf.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/tts-outetts.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/tts/tts.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf-split/gguf-split.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/pydantic_models_to_grammar.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gguf/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gguf/gguf.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/parallel/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/parallel.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/parallel/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama-bench/llama-bench.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/json_schema_to_grammar.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/sycl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/win-build-sycl.bat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/build.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/win-run-llama2.bat  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/ls-sycl-device.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/sycl/run-llama2.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/gbnf-validator.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gbnf-validator/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle.properties  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradlew  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/.gitignore  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/consumer-rules.pro  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/proguard-rules.pro  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/AndroidManifest.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/main/cpp/llama-android.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/llama/build.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.jar  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.properties  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/build.gradle.kts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/settings.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/proguard-rules.pro  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/AndroidManifest.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Color.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Type.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Theme.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/Downloadable.kt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainActivity.kt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_background.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_foreground.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/colors.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/strings.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/values/themes.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher_round.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher.webp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/backup_rules.xml  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/xml/data_extraction_rules.xml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher.webp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.android/app/build.gradle.kts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/quantize/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize/quantize.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/batched-bench.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/batched-bench/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/httplib.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat_screens.webp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/simplechat.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/ui.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/readme.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_simplechat/datautils.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat.mjs  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/system-prompts.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-mangotango.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index-new.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-ketivah.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-snowstorm.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/style.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-playground.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/completion.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/index.js  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/favicon.ico  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-polarnight.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/loading.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/theme-beeninorder.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/colorthemes.css  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/json-schema-to-grammar.mjs  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public_legacy/prompt-formats.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat-llama2.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/server.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/chat.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/utils.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/eslint.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/.prettierignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.app.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tailwind.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/package-lock.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/ChatMessage.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/MarkdownDisplay.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/Header.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/ChatScreen.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/CanvasPyInterpreter.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/Sidebar.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/components/SettingDialog.tsx  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/misc.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/llama-vscode.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/storage.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/common.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/types.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/utils/app.context.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/App.tsx  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/index.scss  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/Config.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/main.tsx  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/webui/src/vite-env.d.ts  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/postcss.config.js  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.node.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/tsconfig.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/vite.config.ts  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/webui/public/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/public/demo-conversation.json  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/webui/package.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/bench/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/bench.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/prometheus.yml  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/bench/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/bench/script.js  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/llama_cpp.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/index.html  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/wild.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/llamapattern.png  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/themes/wild/favicon.ico  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/README.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/buttons_top.png  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/index.html  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/themes/buttons-top/favicon.ico  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/public/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public/index.html.gz  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/public/loading.html  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/tests/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/examples/server/tests/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/tests.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/conftest.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/requirements.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/pytest.ini  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_rerank.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_embedding.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_tokenize.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_completion.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_ctx_shift.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_tool_call.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_lora.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_basic.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_infill.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_chat_completion.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_speculative.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_slot_save.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/unit/test_security.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/server/tests/utils.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/Miku.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-cmake-pkg/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/retrieval/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/retrieval.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/retrieval/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/quantize-stats/quantize-stats.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-13B.bat  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/infill/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/infill.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/infill/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/convert_legacy_llama.py  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/speculative/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/speculative/speculative.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/main/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/main.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/main/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llama.vim  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/lookahead/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/lookahead/lookahead.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/save-load-state.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/save-load-state/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/rpc/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/rpc-server.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/rpc/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/jeopardy.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/qasheet.csv  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/jeopardy/graph.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-vicuna.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/embedding/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/embedding/embedding.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/simple-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/simple-chat/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/deprecation-warning/deprecation-warning.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/gen-docs.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/gen-docs/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/chat-persistent.sh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/examples/perplexity/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/perplexity/perplexity.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/examples/llm.vim  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/requirements/\n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-compare-llama-bench.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-pydantic.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_lora_to_gguf.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_llama_ggml_to_gguf.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/requirements/requirements-test-tokenizer-random.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_hf_to_gguf_update.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-all.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/requirements/requirements-convert_legacy_llama.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/CONTRIBUTING.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Makefile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pyrightconfig.json  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/development/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/debugging-tests.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/HOWTO-add-model.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/idea-arch.key  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/llama-star/idea-arch.pdf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/development/token_generation_performance_tips.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/docs/backend/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/SYCL.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/CANN.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/OPENCL.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/backend/BLIS.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/install.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/docker.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/build.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/llguidance.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/cuda-fedora.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/docs/android.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/cmake/ggml-config.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-rpc/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-hip/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-hip/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/ggml-kompute.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_softmax.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n_pre.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_diagmask.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/common.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_gelu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_silu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rmsnorm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale_8.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_addrow.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_mat_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f16.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_add.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q8_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/rope_common.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-kompute/kompute/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opt.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-alloc.c  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/embed_kernel.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_general.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-opencl/ggml-opencl.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/Doxyfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/.clang-format  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/acl_tensor.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/ascendc_kernels.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/acl_tensor.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/ggml-cann.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cann/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-quants.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-threading.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/cmake/host-toolchain.cmake.in  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_f32.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_from_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/group_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/diag_mask_inf.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_m.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pool2d.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/div.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/acc.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_xs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q8_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q3_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/contig_copy.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_vision.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/count_equal.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/silu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/wkv6.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/im2col.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pad.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sub.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/cos.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/soft_max.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/flash_attn_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_xxs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat_back.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_1.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/leaky_relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_nl.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/relu.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xxs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q6_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/concat.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat_support.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xs.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q3_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argmax.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/timestep_embedding.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu_quick.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argsort.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sin.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/tanh.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_multi.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_split_k_reduce.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/types.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_unary_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/scale.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sum_rows.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_to_quant.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat2_support.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/opt_step_adamw.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_m.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/square.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_0.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/add.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/clamp.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_head.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_s.comp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-vulkan/ggml-vulkan.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend-reg.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal.m  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal-impl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-metal/ggml-metal.metal  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-musa/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-musa/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dequantize.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/ggml-sycl.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gemm.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/common.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/presets.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/concat.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/common.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/softmax.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/convert.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/im2col.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/norm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/vecdotq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmvq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmq.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/outprod.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/tsembd.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dmmv.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/concat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/element_wise.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/norm.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/conv.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/im2col.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/conv.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/wkv6.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gla.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/wkv6.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/tsembd.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/gla.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dpct/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dpct/helper.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/backend.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/element_wise.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/outprod.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/dmmv.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/softmax.hpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/convert.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/rope.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/mmvq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-sycl/rope.hpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/dequantize.cuh  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/musa.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/cuda.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vendors/hip.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sum.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/norm.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmv.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/count-equal.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/binbcast.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/vecdotq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pad.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cp-async.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/wkv6.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/arange.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/softmax.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/wkv6.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/out-prod.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/acc.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/scale.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/clamp.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sumrows.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmvq.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pad.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/quantize.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/diagmask.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argmax.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/getrows.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/count-equal.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f32.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/binbcast.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/rope.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sumrows.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/im2col.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/upscale.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/rope.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/convert.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/sum.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cpy.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/out-prod.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pool2d.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/concat.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/concat.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/quantize.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/upscale.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cpy.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-mma-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/diagmask.cu  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb32.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb8.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb64.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/generate_cu_files.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-cpb16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/softmax.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmvq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmv.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/getrows.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/norm.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/im2col.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/pool2d.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argsort.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argsort.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/convert.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/tsembd.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/common.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/arange.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/clamp.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/tsembd.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/scale.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mma.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/gla.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmq.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/mmq.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/gla.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/unary.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-common.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cuh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/unary.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/acc.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cuda/argmax.cu  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-backend-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/common.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/amx/amx.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cmake/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cmake/FindSIMD.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/cpu-feats-x86.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-threading.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/gguf.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-quants.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-impl.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/ggml-blas.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/src/ggml-blas/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/ggml/include/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-opencl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-sycl.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-kompute.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-blas.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cpu.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-opt.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cpp.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-metal.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-rpc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/gguf.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-vulkan.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-alloc.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-backend.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cuda.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/ggml/include/ggml-cann.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.devops/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/intel.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/vulkan.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/musa.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cuda.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/rocm.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cpp.srpm.spec  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cloud-v-pipeline  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/.devops/nix/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/scope.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/jetson-support.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/sif.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/python-scripts.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/package.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/apps.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/nixpkgs-instances.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/docker.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/devshells.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/nix/package-gguf-py.nix  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/cpu.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cli-cann.Dockerfile  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/llama-cpp-cuda.srpm.spec  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.devops/tools.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/SECURITY.md  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/include/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/include/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/include/llama-cpp.h  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/scripts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml-am.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/gen-unicode-data.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/xxd.cmake  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml.last  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-flags.mk  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/run-all-perf.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/debug-test.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-wikitext-2.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get_chat_template.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-wikitext-103.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/fetch_server_test_models.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/verify-checksum-models.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/build-info.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/run-all-ppl.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/check-requirements.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/sync-ggml.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/ci-run.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-hellaswag.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/gen-authors.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/hf.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/qnt-all.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/compare-llama-bench.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-winogrande.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/get-pg.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/compare-commits.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/scripts/install-oneapi.bat  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/pocs/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/CMakeLists.txt  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/pocs/vdot/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/q8dot.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/pocs/vdot/vdot.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/prompts/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/dan.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-bob.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-baichuan.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/alpaca.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-vicuna-v1.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/reason-act.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/LLM-questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat-with-vicuna-v0.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/assistant.txt  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/prompts/chat-with-qwen.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/parallel-questions.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/dan-modified.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/chat.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/prompts/mnemonics.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.pre-commit-config.yaml  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/Sources/\n",
            "   creating: /content/extracted_folder/content/llama.cpp/Sources/llama/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/Sources/llama/module.modulemap  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/Sources/llama/llama.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/.flake8  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/tests/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-llguidance.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-double-float.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/run-json-schema-to-grammar.mjs  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/tests/get-model.h  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-1-spm.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-backend-ops.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-arg-parser.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-log.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-quantize-perf.cpp  \n",
            " extracting: /content/extracted_folder/content/llama.cpp/tests/.gitignore  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-integration.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-random.py  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-opt.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-autorelease.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/CMakeLists.txt  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-chat.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-0.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-llama-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-quantize-fns.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-sampling.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-barrier.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-rope.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-tokenizer-1-bpe.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-chat-template.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-grammar-parser.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-lora-conversion-inference.sh  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-gguf.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/get-model.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-c.c  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-json-schema-to-grammar.cpp  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/tests/test-model-load-cancel.cpp  \n",
            "   creating: /content/extracted_folder/content/llama.cpp/grammars/\n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/c.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/json_arr.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/chess.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/json.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/README.md  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/english.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/list.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/japanese.gbnf  \n",
            "  inflating: /content/extracted_folder/content/llama.cpp/grammars/arithmetic.gbnf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWvPWg1vAsnQ",
        "outputId": "96944d51-6b53-49d2-ac53-cd1cf6dfd83f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kQIxMHvAuM4",
        "outputId": "51f3f1e7-440c-4fc7-f7b9-3125ef423e7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/extracted_folder/content/llama.cpp/build/bin\n"
      ],
      "metadata": {
        "id": "lXBnYfCSAzUA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnwIhz7lCIqZ",
        "outputId": "e23b0457-baff-4992-8fd7-328054368aea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/extracted_folder/content/llama.cpp/build/bin/libllama.so /usr/lib/\n",
        "!ldconfig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY8nvBmZCKCB",
        "outputId": "2baef540-9849-4215-e0d4-1d31e73c93f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libllama.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PukY--w1CTbo",
        "outputId": "fcdfe481-33b9-451f-8f02-091b11e746a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/extracted_folder -name \"libggml.so\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF6eIhTACZqY",
        "outputId": "e7eeff0c-623c-43e6-d450-5c6650cb5e3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHP0RJ1rCp6A",
        "outputId": "c9656310-74b6-4d22-8a1d-5c1773009ee1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007fff0bb3a000)\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007a3f61ce2000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007a3f61cc2000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007a3f61a99000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007a3f619b0000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007a3f61f35000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/extracted_folder/content/llama.cpp/build/bin\n"
      ],
      "metadata": {
        "id": "NTKTw7ZAC8WI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902VwWcLCqeh",
        "outputId": "f67c9cab-edd8-4070-dae5-854b75f0d77b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/libggml.so\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUsrqk0GDFDp",
        "outputId": "08a002a9-1f54-4cd8-ae8d-016fcd86e212"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007fff587f2000)\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007cebb9d63000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007cebb9d43000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007cebb9b1a000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007cebb9a31000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00007cebb9fb6000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/extracted_folder/content/llama.cpp/build/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe_we9hgEoNi",
        "outputId": "1cf9c485-4012-4790-e384-b9a4888f1198"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-VZmWDmErf6",
        "outputId": "99ed93c4-ba5d-433f-e019-c457fd5ced47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libggml-base.so\t\t       llama-llava-clip-quantize-cli  llama-tts\n",
            "libggml-cpu.so\t\t       llama-lookahead\t\t      llama-vdot\n",
            "libggml.so\t\t       llama-lookup\t\t      test-arg-parser\n",
            "libllama.so\t\t       llama-lookup-create\t      test-autorelease\n",
            "libllava_shared.so\t       llama-lookup-merge\t      test-backend-ops\n",
            "llama-batched\t\t       llama-lookup-stats\t      test-barrier\n",
            "llama-batched-bench\t       llama-minicpmv-cli\t      test-c\n",
            "llama-bench\t\t       llama-parallel\t\t      test-chat\n",
            "llama-cli\t\t       llama-passkey\t\t      test-chat-template\n",
            "llama-convert-llama2c-to-ggml  llama-perplexity\t\t      test-gguf\n",
            "llama-cvector-generator        llama-q8dot\t\t      test-grammar-integration\n",
            "llama-embedding\t\t       llama-quantize\t\t      test-grammar-parser\n",
            "llama-eval-callback\t       llama-quantize-stats\t      test-json-schema-to-grammar\n",
            "llama-export-lora\t       llama-qwen2vl-cli\t      test-llama-grammar\n",
            "llama-gbnf-validator\t       llama-retrieval\t\t      test-log\n",
            "llama-gen-docs\t\t       llama-run\t\t      test-model-load-cancel\n",
            "llama-gguf\t\t       llama-save-load-state\t      test-quantize-fns\n",
            "llama-gguf-hash\t\t       llama-server\t\t      test-quantize-perf\n",
            "llama-gguf-split\t       llama-simple\t\t      test-rope\n",
            "llama-gritlm\t\t       llama-simple-chat\t      test-sampling\n",
            "llama-imatrix\t\t       llama-speculative\t      test-tokenizer-0\n",
            "llama-infill\t\t       llama-speculative-simple       test-tokenizer-1-bpe\n",
            "llama-llava-cli\t\t       llama-tokenize\t\t      test-tokenizer-1-spm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/extracted_folder/content/llama.cpp/build/bin/libggml.so /usr/lib/\n",
        "!ldconfig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL1cku7HDLuB",
        "outputId": "9c0a8ab7-f827-403d-f395-1ab9b26251cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQUQ3Q_EEVSB",
        "outputId": "0cc57949-b5fc-46bf-c8f0-15c465331cac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default: edkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               reasoning format (default: deepseek; allowed values: deepseek, none)\n",
            "                                        controls whether thought tags are extracted from the response, and in\n",
            "                                        which format they're returned. 'none' leaves thoughts unparsed in\n",
            "                                        `message.content`, 'deepseek' puts them in `message.reasoning_content`\n",
            "                                        (for DeepSeek R1 & Command R7B only).\n",
            "                                        only supported for non-streamed responses\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,\n",
            "                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,\n",
            "                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,\n",
            "                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, deepseek3,\n",
            "                                        exaone3, falcon3, gemma, gigachat, glmedge, granite, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, megrez, minicpm,\n",
            "                                        mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch,\n",
            "                                        openchat, orion, phi3, phi4, rwkv-world, vicuna, vicuna-orca, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ldd /content/extracted_folder/content/llama.cpp/build/bin/llama-cli\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTzU7Q2iEck5",
        "outputId": "87ebc50d-5247-4b90-eb33-2999538cf3b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tlinux-vdso.so.1 (0x00007ffc3016f000)\n",
            "\tlibllama.so => /lib/libllama.so (0x00000e8ba16fe000)\n",
            "\tlibggml.so => /lib/libggml.so (0x00000e8ba16ef000)\n",
            "\tlibggml-base.so => not found\n",
            "\tlibstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00000e8ba14c3000)\n",
            "\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00000e8ba13dc000)\n",
            "\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00000e8ba13ba000)\n",
            "\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00000e8ba1191000)\n",
            "\t/lib64/ld-linux-x86-64.so.2 (0x00000e8ba1a00000)\n",
            "\tlibggml-base.so => not found\n",
            "\tlibggml-cpu.so => not found\n",
            "\tlibggml-base.so => not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aشغال"
      ],
      "metadata": {
        "id": "o5dXyO-sE9IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli --help\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiOqHQP5Eh4g",
        "outputId": "65043da1-bb92-4b24-940a-f17e276d3856"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-cpu.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "SZXq1LyJGA0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so /content/extracted_folder/content/llama.cpp/build/bin/llama-cli -m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KCrlsUOE5a5",
        "outputId": "b5962443-e2cc-4b34-84b1-82524c3e2965"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-cpu.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "_5CMCxFUF90E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39DejDWIFYV6",
        "outputId": "693467f5-fd9a-41c3-ed4f-301251a7c205"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-cpu.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAwnOYyELGIb",
        "outputId": "e612c18f-8cdb-49bc-fbb0-04eb414fb94a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ikawrakow/llama-v2-2bit-gguf/resolve/main/llama-v2-70b-q2k.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cEQBsNDK8K7",
        "outputId": "2cfbe3c5-0c1b-4f78-b706-6d8a7cfcf8d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-22 01:01:34--  https://huggingface.co/ikawrakow/llama-v2-2bit-gguf/resolve/main/llama-v2-70b-q2k.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.5, 3.169.137.111, 3.169.137.19, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/2a/0d/2a0d1bc5127a93e843b2aebbcd86242bb02f4a81a4ffe1958a139554aa105c9f/9b6378b4f4418645c751d622759e46d3caa42cd3ae7942b183c0f8a5570aecf1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v2-70b-q2k.gguf%3B+filename%3D%22llama-v2-70b-q2k.gguf%22%3B&Expires=1740189694&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4OTY5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJhLzBkLzJhMGQxYmM1MTI3YTkzZTg0M2IyYWViYmNkODYyNDJiYjAyZjRhODFhNGZmZTE5NThhMTM5NTU0YWExMDVjOWYvOWI2Mzc4YjRmNDQxODY0NWM3NTFkNjIyNzU5ZTQ2ZDNjYWE0MmNkM2FlNzk0MmIxODNjMGY4YTU1NzBhZWNmMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vAkqI0qh3WXiD7TIliJ%7EpgyAW5sT4ZXGMt%7ExmMbj7Ph-zrZgeG%7E0dp2Kg9vO0XVpRckZbLvpTRljEEUNbd6p5nP0Db03rw5Gi59fno5FpBkfaLoWIQXPAZwRXc6ZWNTe7eDUFbZoOvUOs30sCxBVNeSBZbAeKsP4Duj5S06g4EDg62lXJ3Rljra22Pob9meiX20Y5JOB%7Eefq2%7E4HXVMqznq1JeMNACaTik35HT6EbRpwHAfJftWdKNJvjxwjg8sp%7ECQvlu5y2UDXP179vP8Es3y91oObIiGmXuiriVA1mdHtMWnbEe6DeqQ7DE-KqyGIsUxHPI92WFRH7L9wgUyezA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-22 01:01:34--  https://cdn-lfs-us-1.hf.co/repos/2a/0d/2a0d1bc5127a93e843b2aebbcd86242bb02f4a81a4ffe1958a139554aa105c9f/9b6378b4f4418645c751d622759e46d3caa42cd3ae7942b183c0f8a5570aecf1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v2-70b-q2k.gguf%3B+filename%3D%22llama-v2-70b-q2k.gguf%22%3B&Expires=1740189694&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE4OTY5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJhLzBkLzJhMGQxYmM1MTI3YTkzZTg0M2IyYWViYmNkODYyNDJiYjAyZjRhODFhNGZmZTE5NThhMTM5NTU0YWExMDVjOWYvOWI2Mzc4YjRmNDQxODY0NWM3NTFkNjIyNzU5ZTQ2ZDNjYWE0MmNkM2FlNzk0MmIxODNjMGY4YTU1NzBhZWNmMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vAkqI0qh3WXiD7TIliJ%7EpgyAW5sT4ZXGMt%7ExmMbj7Ph-zrZgeG%7E0dp2Kg9vO0XVpRckZbLvpTRljEEUNbd6p5nP0Db03rw5Gi59fno5FpBkfaLoWIQXPAZwRXc6ZWNTe7eDUFbZoOvUOs30sCxBVNeSBZbAeKsP4Duj5S06g4EDg62lXJ3Rljra22Pob9meiX20Y5JOB%7Eefq2%7E4HXVMqznq1JeMNACaTik35HT6EbRpwHAfJftWdKNJvjxwjg8sp%7ECQvlu5y2UDXP179vP8Es3y91oObIiGmXuiriVA1mdHtMWnbEe6DeqQ7DE-KqyGIsUxHPI92WFRH7L9wgUyezA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.38, 3.169.36.120, 3.169.36.128, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22924882976 (21G) [binary/octet-stream]\n",
            "Saving to: ‘llama-v2-70b-q2k.gguf’\n",
            "\n",
            "llama-v2-70b-q2k.gg 100%[===================>]  21.35G  35.7MB/s    in 10m 51s \n",
            "\n",
            "2025-02-22 01:12:26 (33.6 MB/s) - ‘llama-v2-70b-q2k.gguf’ saved [22924882976/22924882976]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "ZwDJKwDNK8ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=\"اسم_المستودع\", repo_type=\"model\")\n"
      ],
      "metadata": {
        "id": "er3Skk8cK8EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"مسار_الملف/اسم_النموذج.gguf\",\n",
        "    path_in_repo=\"اسم_النموذج.gguf\",\n",
        "    repo_id=\"اسم_المستودع\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "x7ubhU7ILfCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=\"المسار_إلى_مجلد_النموذج\",\n",
        "    repo_id=\"اسم_المستودع\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "l4f_vZH6Lgnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFbH64nRMFD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UiL9THgGMFBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRTzmGWvME-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3ce8738f83594779be525e3c17ef92d2",
            "49cb48a3b3cb446d9f587cf9c2563049",
            "0b501236cc054d719eda31c7ed9c4dc6",
            "2f8fd749746245c6a4c8fa654fc95f92",
            "c8aa7649366b447680548b78bc42a25b",
            "aee6ff4d588b4025a8d15f89243b3fa3",
            "ceddb62e0a034776ac9dbcfd66cbebae",
            "588e41f5b6d44f7997af1f3ed46759a7",
            "7df4f1a8a4a8495f9482fadd32593c83",
            "af22f3975fbd4f0dbef8d1bd6671f90c",
            "8e2678c1b1194d6d84ea56254bd2bf50",
            "0fe01a16de6d4659967d5f9297042823",
            "4f5f291019c349d5952f01358aa7ddf7",
            "e2f7a005469d4647999b5b4b176e1725",
            "9394e03a32ba497c9dbb4fecc93381b0",
            "797cf05599464d1984e2d1fc5e6b6e1b",
            "20d6638307f54869a1d7ad6444c5887c",
            "316cdf14f3e14aa2bb3eed1fc4601ea3",
            "9d45f3d53ef84936bba42f4ddbe3ccdf",
            "afade67647544d85b7e481df1143b84d"
          ]
        },
        "id": "CQ-8Ku2vMFfi",
        "outputId": "3cd8ed17-891a-4416-a16f-fefde5d5bddd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ce8738f83594779be525e3c17ef92d2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/llama-v2-70b-q2k.gguf\",\n",
        "    path_in_repo=\"llama-v2-70b-q2k.gguf\",\n",
        "    repo_id=\"rakmik/lama70b2bit\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "c465317f8fef46fba531cacd3778305d",
            "521f0658944f4633849915e33259c67d",
            "935cc44b7d2a4081acded1bbba868a4d",
            "1d6233c9aea8427a9dca6feb5743c423",
            "1efe15bce10241aa9effb8543b4a989e",
            "12ac09e5ba744679835742767983254e",
            "69d9af1f54b24992a1235eeb28d83beb",
            "3eaba38008fb4889b0f822b15537f44d",
            "94582946ef3e404cafd6a29b5aa265ce",
            "36ab45aad7f94487940e2784e3aea820",
            "be8f1274bc7244ff9e212aa8fecdbbbe"
          ]
        },
        "id": "SLqxwhZLMFfj",
        "outputId": "2d31ffbc-86b7-4d58-c499-b8b4406dd454"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-v2-70b-q2k.gguf:   0%|          | 0.00/22.9G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c465317f8fef46fba531cacd3778305d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "HTTP Error 500 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/52/08/5208d7bfc12d2a175d831396c5c2993ceec11f590e9f77aa8bbbe2b7c221f9d1/9b6378b4f4418645c751d622759e46d3caa42cd3ae7942b183c0f8a5570aecf1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250222T011728Z&X-Amz-Expires=86400&X-Amz-Signature=fc4bfcf5dc1d5da3aa43634229b699ba4777003f80aa794c404f596c19560447&X-Amz-SignedHeaders=host&partNumber=433&uploadId=oG6sSi9F.zEHCDZry2lt.XuEvjB86LcHE47avKEa4lSAUTsvbGBIKtdTOruJTxouhOhAwjX6a4WNG0RGzEnqNDFK6m6yAwFQdBZ_x2Oze9hcqjK85AtRUugh4yiHUJBf&x-id=UploadPart\n",
            "WARNING:huggingface_hub.utils._http:HTTP Error 500 thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/52/08/5208d7bfc12d2a175d831396c5c2993ceec11f590e9f77aa8bbbe2b7c221f9d1/9b6378b4f4418645c751d622759e46d3caa42cd3ae7942b183c0f8a5570aecf1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250222T011728Z&X-Amz-Expires=86400&X-Amz-Signature=fc4bfcf5dc1d5da3aa43634229b699ba4777003f80aa794c404f596c19560447&X-Amz-SignedHeaders=host&partNumber=433&uploadId=oG6sSi9F.zEHCDZry2lt.XuEvjB86LcHE47avKEa4lSAUTsvbGBIKtdTOruJTxouhOhAwjX6a4WNG0RGzEnqNDFK6m6yAwFQdBZ_x2Oze9hcqjK85AtRUugh4yiHUJBf&x-id=UploadPart\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rakmik/lama70b2bit/commit/50d851366b7fa5401ef983e2b5657ac37f008c92', commit_message='Upload llama-v2-70b-q2k.gguf with huggingface_hub', commit_description='', oid='50d851366b7fa5401ef983e2b5657ac37f008c92', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rakmik/lama70b2bit', endpoint='https://huggingface.co', repo_type='model', repo_id='rakmik/lama70b2bit'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ikawrakow/llama-v2-2bit-gguf/resolve/main/llama-v2-7b-q2k.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59pF7ideaw7D",
        "outputId": "e9e56e5f-4e11-4244-d4ef-57992d5b1ce7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-22 02:09:56--  https://huggingface.co/ikawrakow/llama-v2-2bit-gguf/resolve/main/llama-v2-7b-q2k.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.169.137.19, 3.169.137.111, 3.169.137.119, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.169.137.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/2a/0d/2a0d1bc5127a93e843b2aebbcd86242bb02f4a81a4ffe1958a139554aa105c9f/debcfef281962ca4c8dbe1acb8e9608a58c46ae7004d5a000e1f7246d189605c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v2-7b-q2k.gguf%3B+filename%3D%22llama-v2-7b-q2k.gguf%22%3B&Expires=1740193796&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE5Mzc5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJhLzBkLzJhMGQxYmM1MTI3YTkzZTg0M2IyYWViYmNkODYyNDJiYjAyZjRhODFhNGZmZTE5NThhMTM5NTU0YWExMDVjOWYvZGViY2ZlZjI4MTk2MmNhNGM4ZGJlMWFjYjhlOTYwOGE1OGM0NmFlNzAwNGQ1YTAwMGUxZjcyNDZkMTg5NjA1Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ue52soZ4wNwH63IXSewYMmJTd%7EVS79g3rc%7EpX%7E-dalQDZPUHbDtRBipJXeje9%7EpgtaIRZVSTp3hVCh%7EhkQZ2jG%7E%7Ep9vbYUaZ%7E%7EmJX%7EOCxy6g4vFvlUpy0RDtxWIwu3zsuWuic%7E8fUwQ-Zlw1x32jnBItE38mjTv0FJ0N62tS5QTwaHu1QCX3uIzUjv2qLy20%7EWnxmLTCEO7%7EyJHlZppWDjV6zhuO01%7EuGJrdxRI5raJxnYmmWlsDUQMvKpcPb3hJv3SLo7PzktjgmfCwRBI39MSjufUjVXfNCRnNG54qX5pQkAx0czmADGvMqrOgwMgV9KEVGHe8m6-7lPqwTjzz3w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-22 02:09:56--  https://cdn-lfs-us-1.hf.co/repos/2a/0d/2a0d1bc5127a93e843b2aebbcd86242bb02f4a81a4ffe1958a139554aa105c9f/debcfef281962ca4c8dbe1acb8e9608a58c46ae7004d5a000e1f7246d189605c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v2-7b-q2k.gguf%3B+filename%3D%22llama-v2-7b-q2k.gguf%22%3B&Expires=1740193796&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE5Mzc5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJhLzBkLzJhMGQxYmM1MTI3YTkzZTg0M2IyYWViYmNkODYyNDJiYjAyZjRhODFhNGZmZTE5NThhMTM5NTU0YWExMDVjOWYvZGViY2ZlZjI4MTk2MmNhNGM4ZGJlMWFjYjhlOTYwOGE1OGM0NmFlNzAwNGQ1YTAwMGUxZjcyNDZkMTg5NjA1Yz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ue52soZ4wNwH63IXSewYMmJTd%7EVS79g3rc%7EpX%7E-dalQDZPUHbDtRBipJXeje9%7EpgtaIRZVSTp3hVCh%7EhkQZ2jG%7E%7Ep9vbYUaZ%7E%7EmJX%7EOCxy6g4vFvlUpy0RDtxWIwu3zsuWuic%7E8fUwQ-Zlw1x32jnBItE38mjTv0FJ0N62tS5QTwaHu1QCX3uIzUjv2qLy20%7EWnxmLTCEO7%7EyJHlZppWDjV6zhuO01%7EuGJrdxRI5raJxnYmmWlsDUQMvKpcPb3hJv3SLo7PzktjgmfCwRBI39MSjufUjVXfNCRnNG54qX5pQkAx0czmADGvMqrOgwMgV9KEVGHe8m6-7lPqwTjzz3w__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.169.36.38, 3.169.36.120, 3.169.36.128, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.169.36.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2277273152 (2.1G) [application/octet-stream]\n",
            "Saving to: ‘llama-v2-7b-q2k.gguf’\n",
            "\n",
            "llama-v2-7b-q2k.ggu 100%[===================>]   2.12G  33.3MB/s    in 65s     \n",
            "\n",
            "2025-02-22 02:11:02 (33.3 MB/s) - ‘llama-v2-7b-q2k.gguf’ saved [2277273152/2277273152]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/llama-v2-7b-q2k.gguf\",\n",
        "    path_in_repo=\"llama-v2-7b-q2k.gguf\",\n",
        "    repo_id=\"rakmik/lama70b2bit\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "b7a02f37056c479f9395c8e1dd742d3c",
            "ca5bd697c60c4f2e977322a25953e532",
            "e08f326ec9214491bfd88ba253b915bd",
            "36d2ac93edc64366a6af890aa2736749",
            "42cc56b2ed674420bd3dfe5dafc22030",
            "c3bf31db040a420a948f82ba5a2826c9",
            "34151d301d30445983cbb173578b9554",
            "3f92940604614bb4ac230efcbf5d443f",
            "a2cce4656a93455bb34ea33d3149f129",
            "ba0820739c4f4d9981f32a5d2bfcba16",
            "40ca005eb9d745468fb062a9078d43fc"
          ]
        },
        "id": "GouoAxjYaycA",
        "outputId": "9b28e561-f7e3-44e7-c2a4-926c8849b71e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-v2-7b-q2k.gguf:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7a02f37056c479f9395c8e1dd742d3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rakmik/lama70b2bit/commit/d35cbf6f3c6c09e7c853e1e453e34aca83048a53', commit_message='Upload llama-v2-7b-q2k.gguf with huggingface_hub', commit_description='', oid='d35cbf6f3c6c09e7c853e1e453e34aca83048a53', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rakmik/lama70b2bit', endpoint='https://huggingface.co', repo_type='model', repo_id='rakmik/lama70b2bit'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2fpj3AdgOZL",
        "outputId": "2d3cd668-ca0c-4771-8de4-1fd88b9b6eb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/llama-v2-7b-q2k.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftZF8Xqkf3SD",
        "outputId": "ad51ec8e-fb6b-427b-b290-a8edbea42086"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4753 (51f311e0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /content/llama-v2-7b-q2k.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = llama2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 2.12 GiB (2.70 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = llama2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size =  2171.07 MiB\n",
            "................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 2048\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 1006660454\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            " Hello, how are you? My name is Rohith, I am from India. округу 3, India. I have just come to your city for 2 months. I am looking for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!LD_PRELOAD=/content/extracted_folder/content/llama.cpp/build/bin/libggml-base.so \\\n",
        "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli \\\n",
        "-m /content/llama-v2-70b-q2k.gguf \\\n",
        "-p \"Hello, how are you?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLCKzwTchT79",
        "outputId": "71d248ef-6dda-45b7-df5e-89643baa255d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4753 (51f311e0) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 723 tensors from /content/llama-v2-70b-q2k.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 28672\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  161 tensors\n",
            "llama_model_loader: - type q2_K:  481 tensors\n",
            "llama_model_loader: - type q4_K:   80 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 21.35 GiB (2.66 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 8192\n",
            "print_info: n_layer          = 80\n",
            "print_info: n_head           = 64\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 28672\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 70B\n",
            "print_info: model params     = 68.98 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_Mapped model buffer size = 21862.14 MiB\n",
            "....................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 2048\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1280.00 MiB\n",
            "llama_init_from_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   584.01 MiB\n",
            "llama_init_from_model: graph nodes  = 2566\n",
            "llama_init_from_model: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 3390921601\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            " Hello, how are you? I’"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://huggingface.co/ikawrakow/llama-v2-2bit-gguf/tree/main"
      ],
      "metadata": {
        "id": "zpBrWWT4i8nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/llama-v2-70b-q2k.gguf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### /content/llama-v2-7b-q2k.gguf"
      ],
      "metadata": {
        "id": "AzT4Ae7oi0jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli -h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB5lXd0JgGqA",
        "outputId": "730f7330-c1ce-400a-ac5f-4d04719773bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/extracted_folder/content/llama.cpp/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KADecaYagJiK",
        "outputId": "0ed2bb62-1705-420b-a5bf-5c0fb634b348"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/extracted_folder/content/llama.cpp/build/bin/llama-cli: error while loading shared libraries: libggml-base.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgl89QfvgdZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
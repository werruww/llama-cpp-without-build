{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9m8wOMimbEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q6_K.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm5OTXFrln8v",
        "outputId": "c791fb49-a0ef-4316-81c2-d68e71fb19fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-21 17:42:27--  https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q6_K.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.45.68, 13.33.45.10, 13.33.45.37, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.45.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/83/6a/836a2383aaf9396df7e51349b13c7700c207710455ae1353ae38fbaa0e4c9cfa/0f4c510daf16e0d1b3bc94931fd9296c28936bebdda2593687d4eb70c5b70628?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-Q6_K.gguf%22%3B&Expires=1740163347&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE2MzM0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzgzLzZhLzgzNmEyMzgzYWFmOTM5NmRmN2U1MTM0OWIxM2M3NzAwYzIwNzcxMDQ1NWFlMTM1M2FlMzhmYmFhMGU0YzljZmEvMGY0YzUxMGRhZjE2ZTBkMWIzYmM5NDkzMWZkOTI5NmMyODkzNmJlYmRkYTI1OTM2ODdkNGViNzBjNWI3MDYyOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=p2glhB08uf5V8kxOgFO%7EzGOATfC2rfJwO598QjvS37509EuwDCNQnl5ZTSBgxmA0BYrElgPPifdrEJTszUHiIDjoka41MkGZ5Zp4k%7ELGgJIuOczcaWglC-4%7EGKNRYAQ9ZIukMFu8zxS4TeDtGS5e-2IMWVeMs1K1-icLVBvrFB%7EDwSdyz47GU7UGmVyT2XdM2kOSPAlLAwThfyjCJlbki2kkVbBJ%7EMROsw6IicZBPshM2JcfyP-wixWQ9qCvmpUoidnbQ8N40m8SdsrFMbklCTvofxsdwOSyxBbrhPdWiU20lZurXoE3%7E9slNtXthnaqzGwGm-768r4cRiDNunGMCA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-21 17:42:28--  https://cdn-lfs-us-1.hf.co/repos/83/6a/836a2383aaf9396df7e51349b13c7700c207710455ae1353ae38fbaa0e4c9cfa/0f4c510daf16e0d1b3bc94931fd9296c28936bebdda2593687d4eb70c5b70628?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Llama-3.2-1B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-1B-Instruct-Q6_K.gguf%22%3B&Expires=1740163347&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDE2MzM0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzgzLzZhLzgzNmEyMzgzYWFmOTM5NmRmN2U1MTM0OWIxM2M3NzAwYzIwNzcxMDQ1NWFlMTM1M2FlMzhmYmFhMGU0YzljZmEvMGY0YzUxMGRhZjE2ZTBkMWIzYmM5NDkzMWZkOTI5NmMyODkzNmJlYmRkYTI1OTM2ODdkNGViNzBjNWI3MDYyOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=p2glhB08uf5V8kxOgFO%7EzGOATfC2rfJwO598QjvS37509EuwDCNQnl5ZTSBgxmA0BYrElgPPifdrEJTszUHiIDjoka41MkGZ5Zp4k%7ELGgJIuOczcaWglC-4%7EGKNRYAQ9ZIukMFu8zxS4TeDtGS5e-2IMWVeMs1K1-icLVBvrFB%7EDwSdyz47GU7UGmVyT2XdM2kOSPAlLAwThfyjCJlbki2kkVbBJ%7EMROsw6IicZBPshM2JcfyP-wixWQ9qCvmpUoidnbQ8N40m8SdsrFMbklCTvofxsdwOSyxBbrhPdWiU20lZurXoE3%7E9slNtXthnaqzGwGm-768r4cRiDNunGMCA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.88.91, 13.33.88.68, 13.33.88.42, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.88.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1021800576 (974M) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-1B-Instruct-Q6_K.gguf?download=true’\n",
            "\n",
            "Llama-3.2-1B-Instru 100%[===================>] 974.46M  23.4MB/s    in 42s     \n",
            "\n",
            "2025-02-21 17:43:10 (23.1 MB/s) - ‘Llama-3.2-1B-Instruct-Q6_K.gguf?download=true’ saved [1021800576/1021800576]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b3821/llama-b3821-bin-ubuntu-x64.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzqg1m5Tmo4Z",
        "outputId": "13510c3d-2ec2-43e6-ef34-be8232fffed2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-21 17:43:10--  https://github.com/ggml-org/llama.cpp/releases/download/b3821/llama-b3821-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/648f225f-3412-4084-8649-d327467297bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250221T174310Z&X-Amz-Expires=300&X-Amz-Signature=b687b4591c7aea274d718ee00864a31a2dd068c82a956a62bf47480e252775ec&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b3821-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-21 17:43:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/648f225f-3412-4084-8649-d327467297bf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250221T174310Z&X-Amz-Expires=300&X-Amz-Signature=b687b4591c7aea274d718ee00864a31a2dd068c82a956a62bf47480e252775ec&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b3821-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63086945 (60M) [application/octet-stream]\n",
            "Saving to: ‘llama-b3821-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b3821-bin-ubu 100%[===================>]  60.16M  12.5MB/s    in 4.7s    \n",
            "\n",
            "2025-02-21 17:43:16 (12.7 MB/s) - ‘llama-b3821-bin-ubuntu-x64.zip’ saved [63086945/63086945]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b3821-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S-zqKDfmpW3",
        "outputId": "eaa4c1f9-2604-4fac-ee67-9ff784060785"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b3821-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/llama-baby-llama  \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-bench-matmult  \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-grad0    \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLOJoUcOmrlJ",
        "outputId": "e3479092-5aa7-482e-a3f9-f08be88061ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512.0)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "-ld,   --logdir LOGDIR                  path under which to save YAML logs (no logging if unset)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefx in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: top_k;tfs_z;typ_p;top_p;min_p;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: 4294967295, use random seed for 4294967295)\n",
            "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: kfypmt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--penalize-nl                           penalize newline tokens (default: false)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--tfs N                                 tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if\n",
            "                                        used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
            "                                        disabled)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted:\n",
            "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     ./build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): ./build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Llama-3.2-1B-Instruct-Q6_K.gguf?download=true -p \"Who is Napoleon Bonaparte?\" -n 256 -ngl 17"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRoK_8ZLmxNf",
        "outputId": "408069ed-d4a3-4410-a5d9-c6d1a2ba82cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 3821 (70392f1f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from /content/Llama-3.2-1B-Instruct-Q6_K.gguf?download=true (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-1B-Instruct-GGU...\n",
            "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
            "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type q6_K:  113 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 16\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 1.24 B\n",
            "llm_load_print_meta: model size       = 967.00 MiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 1B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.07 MiB\n",
            "llm_load_tensors: offloading 16 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 17/17 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   967.00 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 131072\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =  8464.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 518\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "llama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 76796899\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typical -> top-p -> min-p -> temp-ext -> softmax -> dist \n",
            "generate: n_ctx = 131072, n_batch = 2048, n_predict = 256, n_keep = 1\n",
            "\n",
            "Who is Napoleon Bonaparte? A French military leader and statesman, Napoleon Bonaparte is one of the most infamous figures in history, known for his brilliant military strategies, his incredible ambition, and his lasting impact on European politics and history.\n",
            "\n",
            "Napoleon Bonaparte was born on August 15, 1769, in Ajaccio, Corsica. He was the fourth of eleven children born to Carlo Buonaparte and Letizia Ramolino. Napoleon's early life was marked by tragedy, including the loss of his mother and younger sister. This experience would later influence his actions.\n",
            "\n",
            "Napoleon's military career began in 1785, when he joined the French army as an artillery officer. He quickly rose through the ranks, becoming a major in 1793. In 1796, he married Joséphine de Beauharnais, a widow with two children, and she would become a powerful influence on his life.\n",
            "\n",
            "Napoleon's military genius was evident in his victories at the Battle of Austerlitz in 1805 and the Battle of Jena in 1806. He was instrumental in the French conquest of Italy and the invasion of Russia, and his strategic brilliance led to the creation of the Continental System, a blockade of Britain's trade with\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      39.83 ms /   264 runs   (    0.15 ms per token,  6627.67 tokens per second)\n",
            "llama_perf_context_print:        load time =    3647.06 ms\n",
            "llama_perf_context_print: prompt eval time =     566.69 ms /     8 tokens (   70.84 ms per token,    14.12 tokens per second)\n",
            "llama_perf_context_print:        eval time =   35828.09 ms /   255 runs   (  140.50 ms per token,     7.12 tokens per second)\n",
            "llama_perf_context_print:       total time =   36521.45 ms /   263 tokens\n"
          ]
        }
      ]
    }
  ]
}
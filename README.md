# llama-cpp-without-build
run llm with llama cpp without build
